{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: -0.3297407031059265\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "# Load pre-trained word embeddings from a file\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            if len(values) > 1:\n",
    "                word = values[0]\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Function to calculate cosine similarity converted to range [0, 6]\n",
    "# def cosine_similarity(vec1, vec2):\n",
    "#     return cosine(vec1, vec2)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Ensure that both vectors have the same dimensions\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(\"Vectors must be of the same length\")\n",
    "    \n",
    "    # Compute the dot product between the two vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    \n",
    "    # Compute the magnitudes (norms) of the vectors\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Ensure that we do not divide by zero (in case of zero vectors)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0  # No similarity if one of the vectors is zero\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "\n",
    "# def cosine_similarity(vec1, vec2):\n",
    "#     vec1 = vec1.reshape(1, -1)\n",
    "#     vec2 = vec2.reshape(1, -1)\n",
    "#     return sklearn.metrics.pairwise.cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "# Example usage\n",
    "embeddings = load_embeddings(\"datasets/W2V_150.txt\")\n",
    "vec1 = embeddings['sinh_viên']\n",
    "vec2 = embeddings['tân_việt']\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f'Cosine Similarity: {similarity}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION. Evaluating Cosine Similarity Using Pearson and Spearman Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation: 0.3446108828718941\n",
      "Spearman Correlation: 0.2959639633854359\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Load ViSim-400 dataset\n",
    "def load_visim400(file_path):\n",
    "    word_pairs = []\n",
    "    sim_scores = []\n",
    "    line_id = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line_id == 0:\n",
    "                line_id = 1\n",
    "                continue\n",
    "            word1, word2, _, sim1, _, _ = line.split()\n",
    "            word_pairs.append((word1, word2))\n",
    "            sim_scores.append(float(sim1))  # Or use sim2 based on your requirement\n",
    "    return word_pairs, sim_scores\n",
    "\n",
    "# Calculate correlations\n",
    "def evaluate_correlation(embeddings, word_pairs, human_ratings):\n",
    "    calculated_similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in embeddings and word2 in embeddings:\n",
    "            similarity = cosine_similarity(embeddings[word1], embeddings[word2])\n",
    "            calculated_similarities.append(similarity)\n",
    "        else:\n",
    "            calculated_similarities.append(0)  # Handle missing words\n",
    "\n",
    "    # Pearson and Spearman correlations\n",
    "    pearson_corr, _ = pearsonr(calculated_similarities, human_ratings)\n",
    "    spearman_corr, _ = spearmanr(calculated_similarities, human_ratings)\n",
    "    \n",
    "    return pearson_corr, spearman_corr\n",
    "\n",
    "# Load ViSim-400 dataset and evaluate\n",
    "word_pairs, sim_scores = load_visim400(\"Datasets/ViSim-400/Visim-400.txt\")\n",
    "pearson_corr, spearman_corr = evaluate_correlation(embeddings, word_pairs, sim_scores)\n",
    "\n",
    "print(f'Pearson Correlation: {pearson_corr}')\n",
    "print(f'Spearman Correlation: {spearman_corr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3. K-Nearest Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7981586, 'sv'), (0.6784422, 'du_học_sinh'), (0.6702902, 'hs-sv'), (0.661668, 'học_viên'), (0.62819535, 'lưu_học_sinh')]\n"
     ]
    }
   ],
   "source": [
    "import heapq # priority queue\n",
    "\n",
    "# Function to find k most similar words\n",
    "def k_nearest_words(word, embeddings, k=5):\n",
    "    if word not in embeddings:\n",
    "        print(f'{word} not found in embeddings.')\n",
    "        return []\n",
    "\n",
    "    word_vec = embeddings[word]\n",
    "    similarities = []\n",
    "    \n",
    "    for other_word, other_vec in embeddings.items():\n",
    "        if other_word != word:\n",
    "            sim = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((sim, other_word))\n",
    "    \n",
    "    # Get the top-k most similar words\n",
    "    k_nearest = heapq.nlargest(k, similarities, key=lambda x: x[0])\n",
    "    return k_nearest\n",
    "\n",
    "# Example usage\n",
    "nearest_words = k_nearest_words('sinh_viên', embeddings, k=5)\n",
    "print(nearest_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4. Synonym-Antonym Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load synonym-antonym dataset\n",
    "def load_syn_ant_dataset(synpath, antpath, embeddings):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(synpath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word1, word2 = line.split(' ')\n",
    "            # print(word1, '---1-->', word2)\n",
    "            word1 = word1.strip()\n",
    "            word2 =word2.strip()\n",
    "            if word1 in embeddings and word2 in embeddings:\n",
    "                # print(word1, '---1-->', word2)\n",
    "                vec1 = embeddings[word1]\n",
    "                vec2 = embeddings[word2]\n",
    "                # Feature vector can be difference, sum, or concatenation of the two embeddings\n",
    "                feature = np.concatenate([vec1, vec2])  # Example: concatenation\n",
    "                X.append(feature)\n",
    "                y.append(1)  # SYN = 1\n",
    "    f.close()\n",
    "    \n",
    "    with open(antpath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word1, word2 = line.split(' ')\n",
    "            # print(word1, '---2-->', word2)\n",
    "            word1 = word1.strip()\n",
    "            word2 = word2.strip()\n",
    "            if word1 in embeddings and word2 in embeddings:\n",
    "                vec1 = embeddings[word1]\n",
    "                vec2 = embeddings[word2]\n",
    "                # Feature vector can be difference, sum, or concatenation of the two embeddings\n",
    "                feature = np.concatenate([vec1, vec2])  # Example: concatenation\n",
    "                X.append(feature)\n",
    "                y.append(0)  # ANT = 0\n",
    "    f.close()\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "def train_classifier(clf, X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    print(f'Accuracy: {clf.score(X_test, y_test)}')\n",
    "    print(f'Precision: {precision}\\nRecall: {recall}\\nF1: {f1}\\n')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nounpath, verbpath, adjpath, embeddings):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(nounpath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word1, word2, relation = line.split()\n",
    "            # print(word1, '---1-->', word2)\n",
    "            if word1 in embeddings and word2 in embeddings:\n",
    "                vec1 = embeddings[word1]\n",
    "                vec2 = embeddings[word2]\n",
    "                # Feature vector can be difference, sum, or concatenation of the two embeddings\n",
    "                feature = np.concatenate([vec1, vec2])  # Example: concatenation\n",
    "                X.append(feature)\n",
    "                y.append(1 if relation == 'SYN' else 0)  # SYN = 1, ANT = 0\n",
    "    f.close()\n",
    "    with open(verbpath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word1, word2, relation = line.split()\n",
    "            # print(word1, '---2-->', word2)\n",
    "            if word1 in embeddings and word2 in embeddings:\n",
    "                vec1 = embeddings[word1]\n",
    "                vec2 = embeddings[word2]\n",
    "                # Feature vector can be difference, sum, or concatenation of the two embeddings\n",
    "                feature = np.concatenate([vec1, vec2])  # Example: concatenation\n",
    "                X.append(feature)\n",
    "                y.append(1 if relation == 'SYN' else 0)  # SYN = 1, ANT = 0\n",
    "    f.close()\n",
    "    with open(adjpath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word1, word2, relation = line.split()\n",
    "            # print(word1, '---1-->', word2)\n",
    "            if word1 in embeddings and word2 in embeddings:\n",
    "                vec1 = embeddings[word1]\n",
    "                vec2 = embeddings[word2]\n",
    "                # Feature vector can be difference, sum, or concatenation of the two embeddings\n",
    "                feature = np.concatenate([vec1, vec2])  # Example: concatenation\n",
    "                X.append(feature)\n",
    "                y.append(1 if relation == 'SYN' else 0)  # SYN = 1, ANT = 0\n",
    "    f.close()\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "def get_prediction(clf, X, y):\n",
    "    \n",
    "    y_pred = clf.predict(X)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred, average='binary')\n",
    "    print(f'Accuracy: {clf.score(X, y)}')\n",
    "    print(f'Precision: {precision}\\nRecall: {recall}\\nF1: {f1}\\n')\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363522798251093\n",
      "Precision: 0.8621190130624092\n",
      "Recall: 0.9428571428571428\n",
      "F1: 0.9006823351023503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and train\n",
    "X, y = load_syn_ant_dataset(\"antonym-synonym set/Synonym_vietnamese.txt\", \"antonym-synonym set/Antonym_vietnamese.txt\", embeddings)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf = train_classifier(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6751918158567775\n",
      "Precision: 0.592057761732852\n",
      "Recall: 0.9213483146067416\n",
      "F1: 0.7208791208791209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and predict\n",
    "X_new, y_new = load_data('datasets/ViCon-400/400_noun_pairs.txt', 'datasets/ViCon-400/400_verb_pairs.txt', 'datasets/ViCon-400/600_adj_pairs.txt', embeddings)\n",
    "\n",
    "# X_new = scaler.fit_transform(X_new)\n",
    "\n",
    "prediction = get_prediction(clf, X_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9381636477201749\n",
      "Precision: 0.9277818717759764\n",
      "Recall: 0.9992063492063492\n",
      "F1: 0.9621704241497898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spaces = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [10, 100, 1000],\n",
    "    'gamma': [0.1, 0.01, 0.001]\n",
    "}\n",
    "# clf = GridSearchCV(SVC(), spaces, refit=True, verbose=2)\n",
    "\n",
    "clf1 = SVC(kernel='rbf', C=10, gamma=0.01)\n",
    "clf1 = train_classifier(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9624893435635123\n",
      "Precision: 0.9238754325259516\n",
      "Recall: 1.0\n",
      "F1: 0.960431654676259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction1 = get_prediction(clf1, X_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9594003747657714\n",
      "Precision: 0.9671618451915559\n",
      "Recall: 0.9817460317460317\n",
      "F1: 0.974399369830642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf2 = MLPClassifier(hidden_layer_sizes=(300, 100), max_iter=1000, random_state=42)\n",
    "\n",
    "clf2 = train_classifier(clf2, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9846547314578005\n",
      "Precision: 0.9708029197080292\n",
      "Recall: 0.9962546816479401\n",
      "F1: 0.9833641404805915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction2 = get_prediction(clf2, X_new, y_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
